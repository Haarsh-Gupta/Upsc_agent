{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3728129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only downloads the \n",
    "\n",
    "import json\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# 1. Setup\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\") # The current \"bad\" index\n",
    "pc = Pinecone(api_key=api_key)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# 2. Storage for full vector data\n",
    "all_vectors = []\n",
    "\n",
    "print(\"üöÄ Starting Full Vector Backup...\")\n",
    "\n",
    "# 3. Iterate and Fetch\n",
    "# list() gives us IDs, then we fetch the actual vector data\n",
    "for ids in index.list(namespace=\"default\"): \n",
    "    if not ids:\n",
    "        continue\n",
    "    \n",
    "    # fetch() returns the actual dense values and metadata\n",
    "    fetch_response = index.fetch(ids=ids, namespace=\"default\")\n",
    "    \n",
    "    for vector_id, vector_data in fetch_response['vectors'].items():\n",
    "        # We save the exact structure Pinecone expects for an upsert\n",
    "        record = {\n",
    "            \"id\": vector_id,\n",
    "            \"values\": vector_data['values'],           # The OpenAI Embeddings (Free to move!)\n",
    "            \"sparse_values\": vector_data.get('sparse_values'), # Existing BM25 data\n",
    "            \"metadata\": vector_data.get('metadata', {})\n",
    "        }\n",
    "        all_vectors.append(record)\n",
    "\n",
    "# 4. Save to a file\n",
    "output_file = \"full_backup.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_vectors, f)\n",
    "\n",
    "print(f\"‚úÖ Backup complete! Saved {len(all_vectors)} full vectors to '{output_file}'.\")\n",
    "print(\"‚ö†Ô∏è NOW it is safe to delete the index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfebe966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Upse_project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Full Vector Backup...\n",
      "‚úÖ Backup complete! Saved 2126 full vectors to 'full_backup.json'.\n",
      "‚ö†Ô∏è NOW it is safe to delete the index.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# 1. Setup\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "pc = Pinecone(api_key=api_key)\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# 2. Storage for full vector data\n",
    "all_vectors = []\n",
    "\n",
    "print(\"üöÄ Starting Full Vector Backup...\")\n",
    "\n",
    "# Helper function to fix the JSON error\n",
    "def make_serializable(obj):\n",
    "    \"\"\"Converts Pinecone objects to standard Python dicts.\"\"\"\n",
    "    if hasattr(obj, \"to_dict\"):\n",
    "        return obj.to_dict()\n",
    "    return obj\n",
    "\n",
    "# 3. Iterate and Fetch\n",
    "try:\n",
    "    # Get all IDs first\n",
    "    for ids in index.list(namespace=\"default\"): \n",
    "        if not ids:\n",
    "            continue\n",
    "        \n",
    "        # Fetch the actual data\n",
    "        fetch_response = index.fetch(ids=ids, namespace=\"default\")\n",
    "        \n",
    "        # In the new SDK, fetch_response might be an object or dict. \n",
    "        # We access vectors safely.\n",
    "        vectors_dict = fetch_response.vectors if hasattr(fetch_response, \"vectors\") else fetch_response.get(\"vectors\", {})\n",
    "        \n",
    "        for vector_id, vector_data in vectors_dict.items():\n",
    "            \n",
    "            # --- FIX: Convert SparseValues object to a Dict ---\n",
    "            sparse_data = vector_data.get('sparse_values')\n",
    "            if sparse_data:\n",
    "                # If it's the custom object, convert it\n",
    "                sparse_data = make_serializable(sparse_data)\n",
    "\n",
    "            record = {\n",
    "                \"id\": vector_id,\n",
    "                \"values\": vector_data['values'], # Dense vectors usually behave like lists\n",
    "                \"sparse_values\": sparse_data,    # <--- The fixed safe dictionary\n",
    "                \"metadata\": vector_data.get('metadata', {})\n",
    "            }\n",
    "            all_vectors.append(record)\n",
    "\n",
    "    # 4. Save to a file\n",
    "    output_file = \"full_backup.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_vectors, f, indent=None) # indent=None keeps file size smaller\n",
    "\n",
    "    print(f\"‚úÖ Backup complete! Saved {len(all_vectors)} full vectors to '{output_file}'.\")\n",
    "    print(\"‚ö†Ô∏è NOW it is safe to delete the index.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Backup failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tatvalabs-index']\n"
     ]
    }
   ],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "existing_indexes = [i.name for i in pc.list_indexes()]\n",
    "print(existing_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766a7d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé DEBUG INFO:\n",
      "   Target Index Name: 'rag-tatvalabs'\n",
      "   API Key Loaded:    Yes\n",
      "   Existing Indexes:  ['tatvalabs-index']\n",
      "\n",
      "‚ö†Ô∏è Index 'rag-tatvalabs' is MISSING.\n",
      "   Attempting to create it now...\n",
      "‚úÖ SUCCESS: Index 'rag-tatvalabs' created!\n",
      "\n",
      "üìä Index Stats for 'rag-tatvalabs':\n",
      "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '155',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Mon, 15 Dec 2025 10:21:08 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '49',\n",
      "                                    'x-pinecone-request-id': '4502653468299685320',\n",
      "                                    'x-pinecone-request-latency-ms': '49'}},\n",
      " 'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'dotproduct',\n",
      " 'namespaces': {},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. Force Reload .env file (Fixes the \"old name\" issue)\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 2. Get Config\n",
    "API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "TARGET_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "print(f\"üîé DEBUG INFO:\")\n",
    "print(f\"   Target Index Name: '{TARGET_INDEX_NAME}'\")\n",
    "print(f\"   API Key Loaded:    {'Yes' if API_KEY else 'No'}\")\n",
    "\n",
    "# 3. Connect to Pinecone\n",
    "pc = Pinecone(api_key=API_KEY)\n",
    "\n",
    "# 4. List ALL current indexes\n",
    "existing_indexes = [i.name for i in pc.list_indexes()]\n",
    "print(f\"   Existing Indexes:  {existing_indexes}\")\n",
    "\n",
    "# 5. Check Region & Create if missing\n",
    "if TARGET_INDEX_NAME not in existing_indexes:\n",
    "    print(f\"\\n‚ö†Ô∏è Index '{TARGET_INDEX_NAME}' is MISSING.\")\n",
    "    print(\"   Attempting to create it now...\")\n",
    "    \n",
    "    try:\n",
    "        # --- CRITICAL: CHECK YOUR REGION ---\n",
    "        # Look at your Pinecone Console URL or Dashboard.\n",
    "        # It usually says \"region: us-east-1\" or \"us-west-2\".\n",
    "        # Ensure the region below MATCHES your console.\n",
    "        \n",
    "        pc.create_index(\n",
    "            name=TARGET_INDEX_NAME,\n",
    "            dimension=1536,\n",
    "            metric=\"dotproduct\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\") \n",
    "        )\n",
    "        print(f\"‚úÖ SUCCESS: Index '{TARGET_INDEX_NAME}' created!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå CREATION FAILED: {e}\")\n",
    "        print(\"   (Double check if your Pinecone region is 'us-east-1')\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Index '{TARGET_INDEX_NAME}' already exists!\")\n",
    "\n",
    "# 6. Verify Status\n",
    "index = pc.Index(TARGET_INDEX_NAME)\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"\\nüìä Index Stats for '{TARGET_INDEX_NAME}':\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "341c913d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Reading full_backup.json...\n",
      "üöÄ Restoring 2126 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:51<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Restoration Complete! Your Dense (OpenAI) and Sparse (BM25) vectors are active.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "# Configuration\n",
    "API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\") \n",
    "BACKUP_FILE = \"full_backup.json\"\n",
    "\n",
    "def smart_restore():\n",
    "    # 1. Load Backup\n",
    "    if not os.path.exists(BACKUP_FILE):\n",
    "        print(\"‚ùå Backup file not found! Run the backup script first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìñ Reading {BACKUP_FILE}...\")\n",
    "    with open(BACKUP_FILE, \"r\") as f:\n",
    "        vectors = json.load(f)\n",
    "    \n",
    "    # 2. Setup BM25 (Just in case we need to patch missing values)\n",
    "    bm25 = BM25Encoder().default()\n",
    "\n",
    "    # 3. Setup Pinecone & Create Correct Index\n",
    "    pc = Pinecone(api_key=API_KEY)\n",
    "    \n",
    "    # Check if index exists, if not create it with DOTPRODUCT\n",
    "    if INDEX_NAME not in [i.name for i in pc.list_indexes()]:\n",
    "        print(f\"Creating NEW index: {INDEX_NAME} with metric='dotproduct'\")\n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME,\n",
    "            dimension=1536,\n",
    "            metric=\"dotproduct\",  \n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "        )\n",
    "    \n",
    "    index = pc.Index(INDEX_NAME)\n",
    "\n",
    "    # 4. Prepare Batch\n",
    "    batch_size = 50\n",
    "    print(f\"üöÄ Restoring {len(vectors)} items...\")\n",
    "\n",
    "    for i in tqdm.tqdm(range(0, len(vectors), batch_size)):\n",
    "        batch = vectors[i : i + batch_size]\n",
    "        \n",
    "        # --- SAFETY CHECK ---\n",
    "        # Ensure every item has sparse values. \n",
    "        # If the old index lost them, we regenerate them here for free.\n",
    "        for item in batch:\n",
    "            if not item.get(\"sparse_values\") or not item[\"sparse_values\"][\"indices\"]:\n",
    "                text = item[\"metadata\"].get(\"text\", \"\")\n",
    "                if text:\n",
    "                    # Regenerate locally (Free)\n",
    "                    item[\"sparse_values\"] = bm25.encode_documents(text)\n",
    "        \n",
    "        try:\n",
    "            index.upsert(vectors=batch, namespace=\"default\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error on batch {i}: {e}\")\n",
    "\n",
    "    print(\"üéâ Restoration Complete! Your Dense (OpenAI) and Sparse (BM25) vectors are active.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    smart_restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b86383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
