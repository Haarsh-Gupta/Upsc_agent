{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc2d163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Upse_project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import operator\n",
    "from typing import TypedDict, List, Dict, Annotated, Any\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# LangGraph & LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic  import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import google.generativeai as genai\n",
    "from PIL import Image\n",
    "from pinecone import Pinecone\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93886a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is connecting to: rag-tatvalabs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"Python is connecting to: {os.getenv('PINECONE_INDEX_NAME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21c274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_fast = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0, \n",
    ")\n",
    "\n",
    "llm_pro = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-pro\",\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there!\n",
      "\n",
      "As an AI, I don't experience feelings or have a physical state like humans do, but I'm fully operational and ready to assist you! Thanks for asking.\n",
      "\n",
      "How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "res = llm_fast.invoke(\"hello how are you bro\")\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b7c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import tqdm\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Pinecone & OpenAI Imports\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from openai import OpenAI\n",
    "# Sparse Encoding Import\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "class Embedding_model:\n",
    "    def __init__(self, api_key=os.getenv(\"OPENAI_API_KEY\"), model_name=\"text-embedding-3-small\"):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "        print(f\"Embedding model initialized: {model_name}\")\n",
    "\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        response = self.client.embeddings.create(\n",
    "            input=[text],\n",
    "            model=self.model_name\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
    "        response = self.client.embeddings.create(\n",
    "            input=texts,\n",
    "            model=self.model_name\n",
    "        )\n",
    "        return [data.embedding for data in response.data]\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "    def __init__(self, \n",
    "                 api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "                 index_name=os.getenv(\"PINECONE_INDEX_NAME\"),\n",
    "                 embedding_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                 embedding_model_name=\"text-embedding-3-small\",\n",
    "                 namespace=\"default\"):\n",
    "        \n",
    "        self.api_key = api_key\n",
    "        self.index_name = index_name\n",
    "        self.namespace = namespace\n",
    "        \n",
    "        self.embedding_model = Embedding_model(api_key=embedding_api_key, model_name=embedding_model_name)\n",
    "        self.bm25 = BM25Encoder().default()\n",
    "        self.pc = Pinecone(api_key=self.api_key)\n",
    "\n",
    "        if self.index_name not in self.pc.list_indexes().names():\n",
    "            print(f\"Creating index: {self.index_name}\")\n",
    "            self.pc.create_index(\n",
    "                name=self.index_name,\n",
    "                dimension=1536,\n",
    "                metric=\"dotproduct\",\n",
    "                spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "            )\n",
    "\n",
    "        self.index = self.pc.Index(self.index_name)\n",
    "\n",
    "        print(f\"Pinecone setup at index : {self.index_name}\")\n",
    "\n",
    "    def upsert(self, items: List[str], metadata: Dict[str, Any] = None):\n",
    "        \"\"\"\n",
    "        Standard upsert: Generates Dense + Sparse and uploads.\n",
    "        \"\"\"\n",
    "        if not items: raise ValueError(\"Items list is empty!\")\n",
    "        if metadata is None: metadata = {}\n",
    "\n",
    "        print(f'Upserting {len(items)} items...')\n",
    "\n",
    "        for i in tqdm.tqdm(range(0, len(items), self.BATCH_SIZE), desc=\"Uploading Batches\"):\n",
    "            batch_texts = items[i : i + self.BATCH_SIZE]\n",
    "            \n",
    "            try:\n",
    "                dense_vectors = self.embedding_model.get_embeddings_batch(batch_texts)\n",
    "                sparse_vectors = self.bm25.encode_documents(batch_texts)\n",
    "\n",
    "                vectors_to_upsert = []\n",
    "                for j, text in enumerate(batch_texts):\n",
    "                    vector_id = str(uuid.uuid4())\n",
    "                    chunk_metadata = metadata.copy()\n",
    "                    chunk_metadata[\"text\"] = text\n",
    "                    chunk_metadata[\"chunk_index\"] = i + j\n",
    "                    \n",
    "                    vectors_to_upsert.append({\n",
    "                        \"id\": vector_id,\n",
    "                        \"values\": dense_vectors[j],\n",
    "                        \"sparse_values\": sparse_vectors[j],\n",
    "                        \"metadata\": chunk_metadata\n",
    "                    })\n",
    "\n",
    "                self.index.upsert(vectors=vectors_to_upsert, namespace=self.namespace)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error on batch {i}: {e}\")\n",
    "\n",
    "        print(\"âœ… Upload complete.\")\n",
    "\n",
    "    def enrich_sparse_from_metadata(self):\n",
    "        \"\"\"\n",
    "        Auto-Update Function:\n",
    "        1. Iterates through ALL vectors in the namespace.\n",
    "        2. Fetches metadata to find 'text'.\n",
    "        3. Checks if 'sparse_values' is missing or empty.\n",
    "        4. Generates sparse vector and updates the record (keeping dense vectors intact).\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ”„ Scanning namespace '{self.namespace}' for vectors missing sparse data...\")\n",
    "\n",
    "        # 1. Use Pinecone's list pagination to get all IDs\n",
    "        # Note: This generator yields a list of IDs batch by batch\n",
    "        for result in self.index.list(namespace=self.namespace):\n",
    "            batch_ids = result # 'result' is a list of IDs in the new SDK\n",
    "            \n",
    "            if not batch_ids:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # 2. Fetch existing vectors to get metadata and dense values\n",
    "                fetch_response = self.index.fetch(ids=batch_ids, namespace=self.namespace)\n",
    "                fetched_vectors = fetch_response.get('vectors', {})\n",
    "\n",
    "                texts_to_encode = []\n",
    "                ids_to_update = []\n",
    "                existing_dense_values = []\n",
    "                existing_metadatas = []\n",
    "\n",
    "                for _id, data in fetched_vectors.items():\n",
    "                    # --- CHECK: Skip if sparse values already exist ---\n",
    "                    # We check if the key exists and if the indices list is not empty\n",
    "                    current_sparse = data.get('sparse_values', {})\n",
    "                    if current_sparse and current_sparse.get('indices'):\n",
    "                        continue \n",
    "\n",
    "                    # Get text from metadata\n",
    "                    text = data.get('metadata', {}).get('text', \"\")\n",
    "                    \n",
    "                    if text:\n",
    "                        texts_to_encode.append(text)\n",
    "                        ids_to_update.append(_id)\n",
    "                        existing_dense_values.append(data['values'])\n",
    "                        existing_metadatas.append(data.get('metadata', {}))\n",
    "\n",
    "                if not texts_to_encode:\n",
    "                    # Nothing to update in this batch\n",
    "                    continue\n",
    "\n",
    "                print(f\"âš¡ Generating sparse vectors for {len(texts_to_encode)} items...\")\n",
    "\n",
    "                # 3. Generate Sparse Vectors\n",
    "                sparse_vectors = self.bm25.encode_documents(texts_to_encode)\n",
    "\n",
    "                # 4. Prepare Upsert (Modify the existing record)\n",
    "                vectors_to_upsert = []\n",
    "                for j, _id in enumerate(ids_to_update):\n",
    "                    vectors_to_upsert.append({\n",
    "                        \"id\": _id,\n",
    "                        \"values\": existing_dense_values[j],   # PRESERVE existing dense embedding\n",
    "                        \"sparse_values\": sparse_vectors[j],   # ADD new sparse vector\n",
    "                        \"metadata\": existing_metadatas[j]     # PRESERVE metadata\n",
    "                    })\n",
    "\n",
    "                # 5. Upsert back to Pinecone\n",
    "                self.index.upsert(vectors=vectors_to_upsert, namespace=self.namespace)\n",
    "                print(f\"âœ… Updated {len(vectors_to_upsert)} vectors.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error processing batch: {e}\")\n",
    "\n",
    "        print(\"ðŸŽ‰ All vectors checked and enriched.\")\n",
    "\n",
    "    def query(self, user_query: str, top_k: int = 10, filter_metadata: Optional[Dict] = None):\n",
    "        dense_vec = self.embedding_model.get_embedding(user_query)\n",
    "        sparse_vec = self.bm25.encode_queries(user_query)\n",
    "\n",
    "        results = self.index.query(\n",
    "            vector=dense_vec,\n",
    "            sparse_vector=sparse_vec,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            namespace=self.namespace,\n",
    "            filter=filter_metadata\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def delete_by_ids(self, ids: List[str]):\n",
    "        self.index.delete(ids=ids, namespace=self.namespace)\n",
    "        print(f\"âœ… Deleted {len(ids)} vectors.\")\n",
    "\n",
    "    def delete_all(self):\n",
    "        self.index.delete(delete_all=True, namespace=self.namespace)\n",
    "        print(f\"âœ… Deleted all vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e5ed3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model initialized: text-embedding-3-small\n",
      "Pinecone setup at index : rag-tatvalabs\n",
      "QueryResponse(matches=[{'id': 'd85c1485-2e89-4531-bfa0-47e42eca49dd',\n",
      " 'metadata': {'book_name': 'History Class 10',\n",
      "              'chunk_index': 204,\n",
      "              'text': 'serialised important novels, which gave birth to a '\n",
      "                      'particular way of writing novels. In the 1920s in '\n",
      "                      'England, popular works were sold in cheap series, '\n",
      "                      'called the Shilling Series. The dust cover or the book '\n",
      "                      'jacket is also a twentieth-century innovation. With the '\n",
      "                      'onset of the Great Depression in the 1930s, publishers '\n",
      "                      'feared a decline in book purchases. To sustain buying, '\n",
      "                      'they brought out cheap paperback editions.\\n'\n",
      "                      '\\n'\n",
      "                      '# 6 India and the World of Print\\n'\n",
      "                      'Let us see when printing began in India and how ideas '\n",
      "                      'and information were written before the age of print.\\n'\n",
      "                      '\\n'\n",
      "                      '### 6.1 Manuscripts Before the Age of Print\\n'\n",
      "                      'India had a very rich and old tradition of handwritten '\n",
      "                      'manuscripts â€“ in Sanskrit, Arabic, Persian, as well as '\n",
      "                      'in various vernacular languages. Manuscripts were '\n",
      "                      'copied on palm leaves or on handmade paper. Pages were '\n",
      "                      'sometimes beautifully illustrated. They would be either '\n",
      "                      'pressed between wooden covers or sewn together to '\n",
      "                      'ensure preservation. Manuscripts continued to be '\n",
      "                      'produced till well after the introduction of print, '\n",
      "                      'down to the late nineteenth century.\\n'\n",
      "                      'Manuscripts, however, were highly expensive and '\n",
      "                      'fragile. They had to be handled carefully, and they '\n",
      "                      'could not be read easily as the\\n'\n",
      "                      'script'},\n",
      " 'score': 0.916190565,\n",
      " 'values': []}, {'id': '52ee5972-fd81-47ea-9cec-18d7b1039ca0',\n",
      " 'metadata': {'book_name': 'History Class 10',\n",
      "              'chunk_index': 205,\n",
      "              'text': 'century.\\n'\n",
      "                      'Manuscripts, however, were highly expensive and '\n",
      "                      'fragile. They had to be handled carefully, and they '\n",
      "                      'could not be read easily as the\\n'\n",
      "                      'script was written in different styles. So manuscripts '\n",
      "                      'were not widely used in everyday life. Even though '\n",
      "                      'pre-colonial Bengal had developed an extensive network '\n",
      "                      'of village primary schools, students very often did not '\n",
      "                      'read texts. They only learnt to write. Teachers '\n",
      "                      'dictated portions of texts from memory and students '\n",
      "                      'wrote them down. Many thus became literate without ever '\n",
      "                      'actually reading any kinds of texts.\\n'\n",
      "                      '\\n'\n",
      "                      '### 6.2 Print Comes to India\\n'\n",
      "                      'The printing press first came to Goa with Portuguese '\n",
      "                      'missionaries in the mid-sixteenth century. Jesuit '\n",
      "                      'priests learnt Konkani and printed several tracts. By '\n",
      "                      '1674, about 50 books had been printed in the Konkani '\n",
      "                      'and in Kanara languages. Catholic priests printed the '\n",
      "                      'first Tamil book in 1579 at Cochin, and in 1713 the '\n",
      "                      'first Malayalam book was printed by them. By 1710, '\n",
      "                      'Dutch Protestant missionaries had printed 32 Tamil '\n",
      "                      'texts, many of them translations of older works.\\n'\n",
      "                      'The English language press did not grow in India till '\n",
      "                      'quite late even though the English East India Company '\n",
      "                      'began to import presses from the late'},\n",
      " 'score': 0.888273716,\n",
      " 'values': []}, {'id': '941807d1-ba9c-42b5-bc74-e1c2de1249e2',\n",
      " 'metadata': {'book_name': 'History Class 6',\n",
      "              'chunk_index': 4,\n",
      "              'text': 'the frontiers also came into the subcontinent and '\n",
      "                      'settled here.\\n'\n",
      "                      '\\n'\n",
      "                      'These movements of people enriched our cultural '\n",
      "                      'traditions. People have shared new ways of carving '\n",
      "                      'stone, composing music, and even cooking food over '\n",
      "                      'several hundreds of years.\\n'\n",
      "                      '\\n'\n",
      "                      '## Names of the land\\n'\n",
      "                      'Two of the words we often use for our country are India '\n",
      "                      'and Bharat. The word India comes from the Indus, called '\n",
      "                      'Sindhu in Sanskrit. Find Iran and Greece in your atlas. '\n",
      "                      'The Iranians and the Greeks who came through the '\n",
      "                      'northwest about 2500 years ago and were familiar with '\n",
      "                      'the Indus, called it the Hindos or the Indos, and the '\n",
      "                      'land to the east of the river was called India. The '\n",
      "                      'name Bharata was used for a group of people who lived '\n",
      "                      'in the northwest, and who are mentioned in the Rigveda, '\n",
      "                      'the earliest composition in Sanskrit (dated to about '\n",
      "                      '3500 years ago). Later it was used for the country.\\n'\n",
      "                      '\\n'\n",
      "                      '## Finding out about the past\\n'\n",
      "                      'There are several ways of finding out about the past. '\n",
      "                      'One is to search for and read books that were written '\n",
      "                      'long ago. These are called manuscripts, because they '\n",
      "                      'were written by hand (this comes from the Latin word '\n",
      "                      'â€˜manuâ€™, meaning hand). These were usually written on '\n",
      "                      'palm leaf, or on the specially prepared bark'},\n",
      " 'score': 0.847714424,\n",
      " 'values': []}, {'id': '83b1ab40-3cf8-4779-9f85-bce0b0273304',\n",
      " 'metadata': {'book_name': 'History Class 12 (Part 2)',\n",
      "              'chunk_index': 12,\n",
      "              'text': 'very widely in the 1620s, and Shaikh Ali Hazin, who '\n",
      "                      'came to north India in the 1740s. Some of these authors '\n",
      "                      'were fascinated by India, and one of them â€“ Mahmud '\n",
      "                      'Balkhi â€“ even became a sort of sanyasi for a time. '\n",
      "                      'Others such as Hazin were disappointed and even '\n",
      "                      'disgusted with India, where they expected to receive a '\n",
      "                      'red carpet treatment. Most of them saw India as a land '\n",
      "                      'of wonders.\\n'\n",
      "                      '\\n'\n",
      "                      '### 3. FRANÃ‡OIS BERNIER\\n'\n",
      "                      '\\n'\n",
      "                      '## A Doctor with a Difference\\n'\n",
      "                      'Once the Portuguese arrived in India in about 1500, a '\n",
      "                      'number of them wrote detailed accounts regarding Indian '\n",
      "                      'social customs and religious practices. A few of them, '\n",
      "                      'such as the Jesuit Roberto Nobili, even translated '\n",
      "                      'Indian texts into European languages.\\n'\n",
      "                      'Among the best known of the Portuguese writers is '\n",
      "                      'Duarte Barbosa, who wrote a detailed account of trade '\n",
      "                      'and society in south India. Later, after 1600, we find '\n",
      "                      'growing numbers of Dutch, English and French travellers '\n",
      "                      'coming to India. One of the most famous was the French '\n",
      "                      'jeweller Jean-Baptiste Tavernier, who travelled to '\n",
      "                      'India at least six times. He was particularly '\n",
      "                      'fascinated with the trading conditions in India, and '\n",
      "                      'compared India to Iran and the Ottoman empire. Some of '\n",
      "                      'these travellers, like the Italian'},\n",
      " 'score': 0.834418476,\n",
      " 'values': []}, {'id': '40f21b38-4b95-478e-bed9-fa4eaf7ff087',\n",
      " 'metadata': {'book_name': 'History Class 12 (Part 2)',\n",
      "              'chunk_index': 188,\n",
      "              'text': '2.2 The making of manuscripts\\n'\n",
      "                      'All books in Mughal India were manuscripts, that is, '\n",
      "                      'they were handwritten. The centre of manuscript '\n",
      "                      'production was the imperial kitabkhana. Although '\n",
      "                      'kitabkhana can be translated as library, it was a '\n",
      "                      \"scriptorium, that is, a place where the emperor's \"\n",
      "                      'collection of manuscripts was kept and new manuscripts '\n",
      "                      'were produced.\\n'\n",
      "                      'The creation of a manuscript involved a number of '\n",
      "                      'people performing a variety of tasks. Paper makers were '\n",
      "                      'needed to prepare the folios of the manuscript, scribes '\n",
      "                      'or calligraphers to copy the text, gilders to '\n",
      "                      'illuminate the pages, painters to illustrate scenes\\n'\n",
      "                      '\\n'\n",
      "                      '## The flight of the written word\\n'\n",
      "                      \"In Abu'l Fazl's words:\\n\"\n",
      "                      'The written word may embody the wisdom of bygone ages '\n",
      "                      'and may become a means to intellectual progress. The '\n",
      "                      'spoken word goes to the heart of those who are present '\n",
      "                      'to hear it. The written word gives wisdom to those who '\n",
      "                      'are near and far. If it was not for the written word, '\n",
      "                      'the spoken word would soon die, and no keepsake would '\n",
      "                      'be left us from those who are passed away. Superficial '\n",
      "                      'observers see in the letter a dark figure, but the '\n",
      "                      'deep-sighted see in it a lamp of wisdom (chirag-i '\n",
      "                      'shinasai). The written word looks black,'},\n",
      " 'score': 0.801861048,\n",
      " 'values': []}, {'id': '4547b501-b77c-495b-b0a9-70be9e399c50',\n",
      " 'metadata': {'book_name': 'History Class 6',\n",
      "              'chunk_index': 137,\n",
      "              'text': 'were caused by the rotation of the earth on its axis, '\n",
      "                      'even though it seems as if the sun is rising and '\n",
      "                      'setting everyday. He developed a scientific explanation '\n",
      "                      'for eclipses as well. He also found a way of '\n",
      "                      'calculating the circumference of a circle, which is '\n",
      "                      'nearly as accurate as the formula we use today. '\n",
      "                      'Varahamihira, Brahmagupta and Bhaskaracharya were some '\n",
      "                      'other mathematicians and astronomers who made several '\n",
      "                      'discoveries. Try and find out more about them.\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      '# Zero\\n'\n",
      "                      'While numerals had been used earlier, mathematicians in '\n",
      "                      'India now invented a special symbol for zero. This '\n",
      "                      'system of counting was adapted by the Arabs and then '\n",
      "                      'spread to Europe. It continues to be in use throughout '\n",
      "                      'the world.\\n'\n",
      "                      '\\n'\n",
      "                      '## Ayurveda\\n'\n",
      "                      'Ayurveda is a well-known system of health science that '\n",
      "                      'was developed in ancient India. The two famous '\n",
      "                      'practitioners of Ayurveda in ancient India were Charaka '\n",
      "                      '(1st-2nd centuries C.E.) and Sushruta (c. 4th century '\n",
      "                      'C.E.) Charak Samhita, written by Charak is a remarkable '\n",
      "                      'book on medicine. In his treatise, Susruta Samhita, '\n",
      "                      'Sushruta speaks about elaborate surgical procedures.\\n'\n",
      "                      '\\n'\n",
      "                      '## Elsewhere\\n'\n",
      "                      'Paper has become a part of our daily lives. The books '\n",
      "                      'we read are printed on paper, and we use paper for'},\n",
      " 'score': 0.776281118,\n",
      " 'values': []}, {'id': 'bb1808fc-0804-4ddb-a32d-307a79378633',\n",
      " 'metadata': {'book_name': 'History Class 12 (Part 2)',\n",
      "              'chunk_index': 4,\n",
      "              'text': 'he travelled widely in the Punjab and parts of northern '\n",
      "                      'India.\\n'\n",
      "                      'Travel literature was already an accepted part of '\n",
      "                      'Arabic literature by the time he wrote. This literature '\n",
      "                      'dealt with lands as far apart as the Sahara desert in '\n",
      "                      'the west to the River Volga in the north. So, while\\n'\n",
      "                      'few people in India would have read Al-Biruni before '\n",
      "                      '1500, many others outside India may have done so.\\n'\n",
      "                      '\\n'\n",
      "                      '### 1.2 The Kitab-ul-Hind\\n'\n",
      "                      \"Al-Biruni's Kitab-ul-Hind, written in Arabic, is simple \"\n",
      "                      'and lucid. It is a voluminous text, divided into 80 '\n",
      "                      'chapters on subjects such as religion and philosophy, '\n",
      "                      'festivals, astronomy, alchemy, manners and customs, '\n",
      "                      'social life, weights and measures, iconography, laws '\n",
      "                      'and metrology.\\n'\n",
      "                      'Generally (though not always), Al-Biruni adopted a '\n",
      "                      'distinctive structure in each chapter, beginning with a '\n",
      "                      'question, following this up with a description based on '\n",
      "                      'Sanskrit traditions, and concluding with a comparison '\n",
      "                      'with other cultures. Some present-day scholars have '\n",
      "                      'argued that this almost geometric structure, remarkable '\n",
      "                      'for its precision and predictability, owed much to his '\n",
      "                      'mathematical orientation.\\n'\n",
      "                      'Al-Biruni, who wrote in Arabic, probably intended his '\n",
      "                      'work for peoples living along the frontiers of the'},\n",
      " 'score': 0.716842651,\n",
      " 'values': []}, {'id': '7bc09035-f249-42de-9924-87ba3d5b5475',\n",
      " 'metadata': {'book_name': 'History Class 12 (Part 2)',\n",
      "              'chunk_index': 13,\n",
      "              'text': 'fascinated with the trading conditions in India, and '\n",
      "                      'compared India to Iran and the Ottoman empire. Some of '\n",
      "                      'these travellers, like the Italian doctor Manucci, '\n",
      "                      'never returned to Europe, and settled down in India.\\n'\n",
      "                      'FranÃ§ois Bernier, a Frenchman, was a doctor, political '\n",
      "                      'philosopher and historian. Like many others, he came to '\n",
      "                      'the Mughal Empire in search of opportunities. He was in '\n",
      "                      'India for twelve years, from 1656 to 1668, and was '\n",
      "                      'closely associated with the Mughal court, as a '\n",
      "                      'physician to Prince Dara Shukoh, the eldest son of '\n",
      "                      'Emperor Shah Jahan, and later as an intellectual and '\n",
      "                      'scientist, with Danishmand Khan, an Armenian noble at '\n",
      "                      'the Mughal court.\\n'\n",
      "                      '\\n'\n",
      "                      '### 3.1 Comparing â€œEastâ€ and â€œWestâ€\\n'\n",
      "                      'Bernier travelled to several parts of the country, and '\n",
      "                      'wrote accounts of what he saw, frequently comparing '\n",
      "                      'what he saw in India with the situation in Europe. He '\n",
      "                      'dedicated his major writing to Louis XIV, the king of '\n",
      "                      'France, and many of his other works were written in the '\n",
      "                      'form of letters to influential officials and ministers. '\n",
      "                      'In virtually every instance Bernier described what he '\n",
      "                      'saw in India as a bleak situation in comparison to '\n",
      "                      'developments in Europe. As we will see, this assessment '\n",
      "                      'was not always'},\n",
      " 'score': 0.714155495,\n",
      " 'values': []}, {'id': '6a518ea3-5989-4abb-93a4-0f0485ca5945',\n",
      " 'metadata': {'book_name': 'History Class 12 (Part 2)',\n",
      "              'chunk_index': 15,\n",
      "              'text': 'of leather thongs. This double sack likewise contains '\n",
      "                      'the provisions, linen and wearing apparel, both of '\n",
      "                      'master and servants. I have taken care to lay in a '\n",
      "                      \"stock of excellent rice for five or six days' \"\n",
      "                      'consumption, of sweet biscuits flavoured with anise (a '\n",
      "                      'herb), of limes and sugar. Nor have I forgotten a linen '\n",
      "                      'bag with its small iron hook for the purpose of '\n",
      "                      'suspending and draining dahi or curds; nothing being '\n",
      "                      'considered so refreshing in this country as lemonade '\n",
      "                      'and dahi.\\n'\n",
      "                      \"Bernier's works were published in France in 1670-71 and \"\n",
      "                      'translated into English, Dutch, German and Italian '\n",
      "                      'within the next five years. Between 1670 and 1725 his '\n",
      "                      'account was reprinted eight times in French, and by '\n",
      "                      '1684 it had been reprinted three times in English. This '\n",
      "                      'was in marked contrast to the accounts in Arabic and '\n",
      "                      'Persian, which circulated as manuscripts and were '\n",
      "                      'generally not published before 1800.\\n'\n",
      "                      '\\n'\n",
      "                      '## The creation and circulation of ideas about India\\n'\n",
      "                      'The writings of European travellers helped produce an '\n",
      "                      'image of India for Europeans through the printing and '\n",
      "                      'circulation of their books. Later, after 1750, when '\n",
      "                      'Indians like Shaikh Itisamuddin and Mirza Abu Talib '\n",
      "                      'visited Europe and confronted this image that'},\n",
      " 'score': 0.710920572,\n",
      " 'values': []}, {'id': '2db23b8f-052e-45de-919f-99af2a108513',\n",
      " 'metadata': {'book_name': 'History Class 10',\n",
      "              'chunk_index': 221,\n",
      "              'text': 'on colonial misrule and encouraged nationalist '\n",
      "                      'activities. Attempts to throttle nationalist criticism '\n",
      "                      'provoked militant protest. This in turn led to a '\n",
      "                      'renewed cycle of persecution and protests. When Punjab '\n",
      "                      'revolutionaries were deported in 1907, Balgangadhar '\n",
      "                      'Tilak wrote with great sympathy about them in his '\n",
      "                      'Kesari. This led to his imprisonment in 1908, provoking '\n",
      "                      'in turn widespread protests all over India.\\n'\n",
      "                      '\\n'\n",
      "                      '## Box 4\\n'\n",
      "                      'Sometimes, the government found it hard to find '\n",
      "                      'candidates for editorship of loyalist papers. When '\n",
      "                      'Sanders, editor of the Statesman that had been founded '\n",
      "                      'in 1877, was approached, he asked rudely how much he '\n",
      "                      'would be paid for suffering the loss of freedom. The '\n",
      "                      'Friend of India refused a government subsidy, fearing '\n",
      "                      'that this would force it to be obedient to government '\n",
      "                      'commands.\\n'\n",
      "                      '\\n'\n",
      "                      '## Box 5\\n'\n",
      "                      'The power of the printed word is most often seen in the '\n",
      "                      'way governments seek to regulate and suppress print. '\n",
      "                      'The colonial government kept continuous track of all '\n",
      "                      'books and newspapers published in India and passed '\n",
      "                      'numerous laws to control the press.\\n'\n",
      "                      'During the First World War, under the Defence of India '\n",
      "                      'Rules, 22 newspapers had to furnish securities. Of '\n",
      "                      'these, 18 shut down rather than comply'},\n",
      " 'score': 0.701455653,\n",
      " 'values': []}], namespace='default', usage={'read_units': 1}, _response_info={'raw_headers': {'date': 'Mon, 15 Dec 2025 10:38:55 GMT', 'content-type': 'application/json', 'content-length': '13656', 'connection': 'keep-alive', 'x-pinecone-max-indexed-lsn': '62', 'x-pinecone-request-latency-ms': '332', 'x-pinecone-request-id': '4521009319127188944', 'x-envoy-upstream-service-time': '103', 'grpc-status': '0', 'server': 'envoy'}})\n"
     ]
    }
   ],
   "source": [
    "store = VectorStore()\n",
    "res = store.query(\"which books were written in the India \")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a7a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res.matches))\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m res.matches:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "print(len(res.matches))\n",
    "\n",
    "for data in res.matches:\n",
    "    print(data.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395e530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import DictCursor\n",
    "import cloudinary\n",
    "import cloudinary.uploader\n",
    "import cloudinary.api\n",
    "\n",
    "NEON_DB_URL = os.getenv(\"DATABASE_URL\")\n",
    "CLOUD_NAME = os.getenv(\"CLOUD_NAME\")\n",
    "CLOUD_API_KEY = os.getenv(\"CLOUD_API_KEY\")\n",
    "CLOUD_API_SECRET = os.getenv(\"CLOUD_API_SECRET\")\n",
    "\n",
    "class DbStorage():\n",
    "    def __init__(self, db_url= os.getenv(\"DATABASE_URL\")):\n",
    "        self.db_url = db_url\n",
    "        self.conn = psycopg2.connect(NEON_DB_URL)\n",
    "        cur = self.conn.cursor()\n",
    "        \n",
    "        # Using UUID as PRIMARY KEY\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS images (\n",
    "                id UUID PRIMARY KEY,\n",
    "                image_url TEXT NOT NULL,\n",
    "                description TEXT,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.conn.commit()\n",
    "        cur.close()\n",
    "        self.conn.close()\n",
    "\n",
    "    def insert(self, id: str, image_url: str, description: str = None):\n",
    "        self.conn = psycopg2.connect(NEON_DB_URL)\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO images (id, image_url, description)\n",
    "            VALUES (%s, %s, %s)\n",
    "        \"\"\", (id, image_url, description))\n",
    "        self.conn.commit()\n",
    "        cur.close()\n",
    "        self.conn.close()\n",
    "\n",
    "    def get(self, id: str) -> Dict[str, Any]:\n",
    "        self.conn = psycopg2.connect(NEON_DB_URL)\n",
    "        cur = self.conn.cursor(cursor_factory=DictCursor)\n",
    "        cur.execute(\"SELECT * FROM images WHERE id = %s\", (id,))\n",
    "        row = cur.fetchone()\n",
    "        cur.close()\n",
    "        self.conn.close()\n",
    "        if row:\n",
    "            return dict(row)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def delete(self, id: str):\n",
    "        self.conn = psycopg2.connect(NEON_DB_URL)\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(\"DELETE FROM images WHERE id = %s\", (id,))\n",
    "        self.conn.commit()\n",
    "        cur.close()\n",
    "        self.conn.close()\n",
    "\n",
    "class CloudStorage():\n",
    "    def __init__(self, cloud_name= os.getenv(\"CLOUD_NAME\"), \n",
    "                api_key=os.getenv(\"CLOUD_API_KEY\"), \n",
    "                api_secret=os.getenv(\"CLOUD_API_SECRET\")):\n",
    "        \n",
    "        self.cloud_name = cloud_name\n",
    "        self.api_key = api_key\n",
    "        self.api_secret = api_secret\n",
    "\n",
    "        cloudinary.config( \n",
    "        cloud_name = self.cloud_name,\n",
    "        api_key = self.api_key,\n",
    "        api_secret = self.api_secret,\n",
    "        secure = True\n",
    "        )\n",
    "\n",
    "    def upload(self , images: List[str]) -> List[List[str]]:\n",
    "        res = []\n",
    "        for img_path in images:\n",
    "            try:\n",
    "                id = str(uuid.uuid4())\n",
    "                response = cloudinary.uploader.upload(img_path)\n",
    "                res.append([id, response['secure_url']])\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to upload {img_path}: {e}\")\n",
    "        return res\n",
    "    \n",
    "    def delete(self, public_id: str) -> bool:\n",
    "        try:\n",
    "            response = cloudinary.uploader.destroy(public_id)\n",
    "            if response.get('result') == 'ok':\n",
    "                print(f\"âœ… Deleted image with public_id: {public_id}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"âŒ Failed to delete image with public_id: {public_id}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error deleting image {public_id}: {e}\")\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "509e6146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Upse_project\\Images\\essay.png\n"
     ]
    }
   ],
   "source": [
    "image = os.path.join(os.getcwd() , \"../Images/essay.png\")\n",
    "image = os.path.abspath(image) \n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3755e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field \n",
    "from typing import List , Annotated , Optional , Literal\n",
    "\n",
    "class State(BaseModel):\n",
    "    answer_sheet : List[str] = Field(description=\"User's uploaded files\")\n",
    "    images_url : Optional[Dict[List[str]]] = None\n",
    "    \n",
    "    user_question : str = Field(description=\"The question asked by the user\")\n",
    "    user_answer : str =  Field(description=\"Attempted answer to the question by the user\")\n",
    "\n",
    "    subject : Literal[\"GS1\" , \"GS2\" , \"GS3\" , \"GS4\"] = Field(default= \"GS1\" , description= \"Selected subject by the user\")\n",
    "    extracted_keypoints : Annotated[List[str], operator.add , Field(description=\"All possible key points extracted from the users question\")]\n",
    "    reterived_context : Annotated[Optional[List[str]], operator.add , Field(description=\"All possible context reterived from the vector database\")]\n",
    "    policies_keywords : Annotated[List[str] , operator.add , Field(description=\"Keywords to search the realted policies\")]\n",
    "    related_policies : Annotated[List[str] , operator.add , Field(description=\"Various Government Policis related to the points\")]\n",
    "\n",
    "    grammer_report : str\n",
    "    content_gap_report : str\n",
    "    feedback_summary : str\n",
    "\n",
    "    total_score : Literal[10 , 15 , 20]\n",
    "    final_score : float\n",
    "    model_answer : str \n",
    "\n",
    "# GS Subject Mapping for Context\n",
    "GS_SYLLABUS = {\n",
    "    \"GS1\": \"Indian Heritage and Culture, History and Geography of the World and Society.\",\n",
    "    \"GS2\": \"Governance, Constitution, Polity, Social Justice and International relations.\",\n",
    "    \"GS3\": \"Technology, Economic Development, Bio diversity, Environment, Security and Disaster Management.\",\n",
    "    \"GS4\": \"Ethics, Integrity and Aptitude.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832277cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParse , PydanticOutputParser\n",
    "from langchain.messages import SystemMessage\n",
    "\n",
    "class OcrModel(BaseModel):\n",
    "    question: str = Field(description=\"The question asked in the exam paper.\")\n",
    "    answer: str = Field(description=\"The student's answer text.\")\n",
    "\n",
    "class OcrContent(BaseModel):\n",
    "    content : str = Field(description=\"Extract the content of the image\")\n",
    "\n",
    "class KeyPoints(BaseModel):\n",
    "    keypoints: List[str] = Field(description=\"Key concepts required for the answer.\")\n",
    "\n",
    "class PoliciesKeywords(BaseModel):\n",
    "    keywords: List[str] = Field(description=\"Search queries for government policies.\")\n",
    "\n",
    "class GradingReport(BaseModel):\n",
    "    score: float = Field(description=\"The score awarded.\")\n",
    "    feedback: str = Field(description=\"Detailed feedback explanation.\")\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    content_gap_report : str\n",
    "    feedback_summary : str \n",
    "    final_score : float\n",
    "\n",
    "first_ocr_parser = PydanticOutputParser(pydantic_object=OcrModel)\n",
    "second_ocr_parser = PydanticOutputParser(pydantic_object=OcrContent)\n",
    "keypoint_parse = PydanticOutputParser(pydantic_object=KeyPoints)\n",
    "policies_parse = PydanticOutputParser(pydantic_object= PoliciesKeywords)\n",
    "grade_parser = PydanticOutputParser(pydantic_object= GradingReport)\n",
    "evaluation_parser = PydanticOutputParser(pydantic_object= Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48703b67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m first_ocr_prompt = \u001b[43mPromptTemplate\u001b[49m(\n\u001b[32m      2\u001b[39m     template = \u001b[33m'''\u001b[39m\u001b[33mYou are an expert Optical Character Recognition (OCR) system for handwritten documents.\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33m            Analyze this image of a student\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms essay.\u001b[39m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[33m            Task:\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m            1. Identify the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mQuestion\u001b[39m\u001b[33m'\u001b[39m\u001b[33m of the article. If no question is clear, infer a relevant topic question .\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m            2. Transcribe the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mBody\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mAnswer\u001b[39m\u001b[33m'\u001b[39m\u001b[33m of the article exactly as written. Fix obvious spelling errors but keep the grammar as is (to evaluate later).\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m            \u001b[39m\u001b[38;5;132;01m{formate_instruction}\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33m            \u001b[39m\u001b[33m'''\u001b[39m,\n\u001b[32m     10\u001b[39m             partial_variables=[{\u001b[33m\"\u001b[39m\u001b[33mformate_instruction\u001b[39m\u001b[33m\"\u001b[39m: first_ocr_parser.get_format_instructions}]\n\u001b[32m     11\u001b[39m         )\n\u001b[32m     13\u001b[39m second_ocr_prompt = PromptTemplate(\n\u001b[32m     14\u001b[39m     template= \u001b[33m\"\u001b[39m\u001b[33mExtract the content of the image\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     partial_variables=[{\u001b[33m\"\u001b[39m\u001b[33mformate_instruction\u001b[39m\u001b[33m\"\u001b[39m: second_ocr_parser.get_format_instructions}]\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m keypoint_prompt = PromptTemplate.from_template(\n\u001b[32m     19\u001b[39m     template = \u001b[33m\"\u001b[39m\u001b[33mSubject: \u001b[39m\u001b[38;5;132;01m{subject_desc}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerate key concepts/facts/articles that MUST be in the answer. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{formate_instruction}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m     input_variables = [\u001b[33m'\u001b[39m\u001b[33msubject_desc\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     21\u001b[39m     partial_variables=[{\u001b[33m\"\u001b[39m\u001b[33mformate_instruction\u001b[39m\u001b[33m\"\u001b[39m: keypoint_parse.get_format_instructions}]\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "first_ocr_prompt = PromptTemplate(\n",
    "    template = '''You are an expert Optical Character Recognition (OCR) system for handwritten documents.\n",
    "            Analyze this image of a student's essay.\n",
    "            \n",
    "            Task:\n",
    "            1. Identify the 'Question' of the article. If no question is clear, infer a relevant topic question .\n",
    "            2. Transcribe the 'Body' or 'Answer' of the article exactly as written. Fix obvious spelling errors but keep the grammar as is (to evaluate later).\n",
    "            {formate_instruction}\n",
    "            ''',\n",
    "            partial_variables={\"formate_instruction\": first_ocr_parser.get_format_instructions()}\n",
    "        )\n",
    "\n",
    "second_ocr_prompt = PromptTemplate(\n",
    "    template= \"Extract the content of the image\",\n",
    "    partial_variables={\"formate_instruction\": second_ocr_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "keypoint_prompt = PromptTemplate(\n",
    "    template = \"Subject: {subject_desc}\\nQuestion: {question}\\nGenerate key concepts/facts/articles that MUST be in the answer. \\n {formate_instruction}\",\n",
    "    input_variables = ['subject_desc', 'question'],\n",
    "    partial_variables={\"formate_instruction\": keypoint_parse.get_format_instructions()}\n",
    "    \n",
    ")\n",
    "\n",
    "policies_keyword_prompt = PromptTemplate(\n",
    "    template = \"Generate 3 precise search queries for Indian Govt schemes related to: {keypoints}\",\n",
    "    input_variables = ['keypoints'],\n",
    "    partial_variables={\"formate_instruction\": policies_parse.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_answer_prompt = PromptTemplate(\n",
    "    template= '''\n",
    "    Max score: {total_score}\n",
    "    Subject: {subject_desc}\n",
    "    Question: {question} \n",
    "    Context: {keypoints} \n",
    "    Govt Policies: {policies} \n",
    "    1. Draft a imporved answer of the student answer\n",
    "    of the user answer \n",
    "    {answer} \n",
    "    2. Given feedback of the student answer\n",
    "    {feedback} \n",
    "    ''',\n",
    "    input_variables = ['total_score' , 'subject_desc' , 'question' , 'keypoints' , 'policies' , 'answer' , 'feedback']\n",
    ")\n",
    "\n",
    "\n",
    "grammer_report_prompt = PromptTemplate(\n",
    "    template = \"You are a strict UPSC Examiner for {subject}\\n Check the grammer of the student answer \\n\\n {answer}\",\n",
    "    input_variables = ['subject' , 'answer']\n",
    ")\n",
    "\n",
    "content_report_prompt = PromptTemplate(\n",
    "    template = \"\"\"You are a strict UPSC Examiner for {subject}.\n",
    "    \n",
    "    Question: {question}\n",
    "    Max score: {total_score}\n",
    "\n",
    "    Policies : {policies}\n",
    "    KeyPoints : {keypoints}\n",
    "    \n",
    "    Student Answer: {user_answer}\n",
    "\n",
    "    Grammer_Peport : {grammer_report}\n",
    "\n",
    "    1. Evaluate the answer based on the Subject.\n",
    "    2. Provide a score out of {total_score}.\n",
    "    3. Give detailed feedback.\n",
    "    4. Give the detailed content_gap_feedback \\n\n",
    "    {formate_instructions}\n",
    "    \"\"\",\n",
    "    input_variables = ['question' , 'total_score' , 'policies' , 'keypoints' , 'user_answer' , 'grammer_report' ],\n",
    "    partial_variables={\"formate_instruction\": evaluation_parser.get_format_instructions()}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0abb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from tavily import TavilyClient\n",
    "\n",
    "def extract_uuid(text_list : List):\n",
    "    \"\"\"\n",
    "    Extracts UUIDs from text formatted as <img = UUID>.\n",
    "    Example: <img = 77b0bee1-fb60-4da9-8ef0-a837cd476691>\n",
    "    \"\"\"\n",
    "    uuids = []\n",
    "    # Pattern matches: <img = (uuid-like-string) >\n",
    "    pattern = r'<img\\s*=\\s*([a-f0-9\\-]+)>'\n",
    "    \n",
    "    for text in text_list:\n",
    "        if text:\n",
    "            matches = re.findall(pattern, text)\n",
    "            uuids.extend(matches)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    return list(dict.fromkeys(uuids))\n",
    "\n",
    "def rag_content(state : State):\n",
    "    \"\"\"Retrieves text context AND extracts embedded Image UUIDs.\"\"\"\n",
    "    print(\"--- RAG Node ---\")\n",
    "    store = VectorStore()\n",
    "    db = DbStorage()\n",
    "    \n",
    "    context_texts = []\n",
    "    image_urls = []\n",
    "    \n",
    "    # 1. Retrieve Text Chunks\n",
    "    for point in state['extracted_keypoints']:\n",
    "        # Query Pinecone\n",
    "        results = store.query(point, top_k=2)\n",
    "        for match in results['matches']:\n",
    "            text = match['metadata'].get('text', '')\n",
    "            context_texts.append(text)\n",
    "    \n",
    "    # 2. Extract UUIDs from the Retrieved Text\n",
    "    # The text contains <img = uuid> tags\n",
    "    img_ids = extract_uuid(context_texts)\n",
    "    \n",
    "    # 3. Fetch Actual URLs from Postgres\n",
    "    for iid in img_ids:\n",
    "        img_record = db.get_image(iid)\n",
    "        if img_record and 'image_url' in img_record:\n",
    "            image_urls.append(img_record['image_url'])\n",
    "            \n",
    "    print(f\"Found {len(img_ids)} image references, resolved {len(image_urls)} URLs.\")\n",
    "            \n",
    "    return {\n",
    "        \"retrieved_context\": context_texts,\n",
    "        \"related_image_urls\": image_urls\n",
    "    }\n",
    "\n",
    "\n",
    "def ocr_node(state: State ):\n",
    "    \"\"\"\n",
    "    Handles multiple images/PDF pages.\n",
    "    Concatenates findings into a single question/answer context.\n",
    "    \"\"\"\n",
    "    files = state['answer_sheet']\n",
    "    print(f\"--- OCR Node: Processing {len(files)} files ---\")\n",
    "    \n",
    "    full_answer_text = \"\"\n",
    "    detected_question = \"\"\n",
    "    \n",
    "    # Logic: \n",
    "    # 1. Load all images.\n",
    "    # 2. Send to Gemini Vision.\n",
    "\n",
    "    \n",
    "    for i , file in enumerate(files):\n",
    "        if i == 0:\n",
    "            res = first_ocr_prompt | llm_fast.with_structured_output(first_ocr_parser)\n",
    "            detected_question = res['question']\n",
    "            full_answer_text = res['answer']\n",
    "        else:\n",
    "            res = second_ocr_prompt | llm_fast.with_structured_output(second_ocr_parser)\n",
    "            full_answer_text += res['content']\n",
    "   \n",
    "    return {\n",
    "        \"user_question\": detected_question,\n",
    "        \"user_answer\": full_answer_text\n",
    "    }\n",
    "\n",
    "def keypoint_generation(state: State):\n",
    "    \"\"\"Generates expected keypoints based on Question & Subject.\"\"\"\n",
    "    print(\"--- Keypoint Gen ---\")\n",
    "    \n",
    "    subject_context = GS_SYLLABUS.get(state['subject'], \"General Studies\")\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"Subject: {subject_desc}\\nQuestion: {question}\\nGenerate 5-7 key concepts/facts/articles that MUST be in the answer.\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm_fast.with_structured_output(KeyPoints)\n",
    "    res = chain.invoke({\n",
    "        \"subject_desc\": subject_context,\n",
    "        \"question\": state['user_question']\n",
    "    })\n",
    "    \n",
    "    return {\"extracted_keypoints\": res.keypoints}\n",
    "\n",
    "\n",
    "def polices_search(state: State):\n",
    "    \"\"\"Searches for Govt Policies using Tavily.\"\"\"\n",
    "    print(\"--- Policy Search ---\")\n",
    "    tavily = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "    \n",
    "    kw_chain = policies_keyword_prompt | llm_fast.with_structured_output(PoliciesKeywords)\n",
    "    kw_res = kw_chain.invoke({\"keypoints\": state['extracted_keypoints']})\n",
    "    \n",
    "    policies = []\n",
    "    if kw_res and kw_res.keywords:\n",
    "        for query in kw_res.keywords:\n",
    "            try:\n",
    "                # Search Tavily\n",
    "                search_result = tavily.search(query=query, search_depth=\"basic\", max_results=1)\n",
    "                for res in search_result['results']:\n",
    "                    policies.append(f\"Policy: {res['title']} - {res['content'][:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Tavily Error: {e}\")\n",
    "            \n",
    "    return {\"related_policies\": policies}\n",
    "\n",
    "def draft_model_answer(state: State):\n",
    "    print(\"--- Drafting Analysis ---\")\n",
    "   \n",
    "    chain = model_answer_prompt | llm_pro | StrOutputParse\n",
    "    chain.invoke({\n",
    "        'total_score' : state['total_score'], \n",
    "        'subject_desc' :f\" {state['subject']} \\n {GS_SYLLABUS.get(state['subject'])}) \" ,\n",
    "        'question' : state['user_question'],\n",
    "        'keypoints' : state['keypoint'], \n",
    "        'policies' : state['related_policies'] ,\n",
    "        'answer' : state['user_answer'],\n",
    "        'feedback' : state['feedback']\n",
    "    })\n",
    "    return {\"model_answer_draft\": chain.content}\n",
    "\n",
    "def grammer_report (state : State):\n",
    "    print(\"--- Grammer Analysis ---\")\n",
    "    chain = grammer_report_prompt | llm_fast | StrOutputParse\n",
    "    res = chain.invoke({'subject' : state['subject'] ,\n",
    "                  'answer' : state['user_answer']})\n",
    "    return {\"grammer_report\" : res}\n",
    "\n",
    "def final_report (state : State):\n",
    "    print(\"--- Content Report Analysis ---\")\n",
    "    chain = content_report_prompt | llm_fast | evaluation_parser \n",
    "    chain.invoke({\n",
    "        'total_score' : state['total_score'], \n",
    "        'subject' :f\" {state['subject']} \\n {GS_SYLLABUS.get(state['subject'])}) \" ,\n",
    "        'question' : state['user_question'],\n",
    "        'keypoints' : state['keypoint'], \n",
    "        'policies' : state['related_policies'] ,\n",
    "        'answer' : state['user_answer'],\n",
    "        'grammmer_report' : state['grammmer_report']\n",
    "    })\n",
    "\n",
    "    return {'final_score' : chain['final_score'],\n",
    "            'content_gap_report' : chain['content_gap_report'],\n",
    "            'feedback_summary' : chain['feedback_summary']}\n",
    "\n",
    "def input_console(state : State , files: List[str]):\n",
    "    subs = ['GS1' , 'GS2' , 'GS3' , 'GS4']\n",
    "    total_score = int(input(\"Enter the total_score eg: 10 15 20\"))\n",
    "\n",
    "    print(\"select the subject\")\n",
    "    print(GS_SYLLABUS)\n",
    "    for i , sub in enumerate(subs):\n",
    "        print(f\"{i} => {sub}\")\n",
    "\n",
    "    subject = int(input(\"press the number to select the subject\"))\n",
    "    subject = subs[subject]\n",
    "\n",
    "    return {\n",
    "        'subject' : subject ,\n",
    "        'total_score' : total_score,\n",
    "        'answer_sheet' : files\n",
    "    }\n",
    "\n",
    "def routing_condition() -> bool:\n",
    "    generate = input(\"y/es and n/o\")\n",
    "    generate = True if generate == 'y' else False \n",
    "    return generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "#nodes \n",
    "workflow.add('input' , input_console)\n",
    "workflow.add_node('ocr' , ocr_node)\n",
    "workflow.add_node('keypoint_gen' ,  keypoint_generation)\n",
    "workflow.add_node('policy_search' , polices_search)\n",
    "workflow.add_node('grammer_report' , grammer_report)\n",
    "workflow.add_node('final_report' , final_report)\n",
    "workflow.add_node('rag' , rag_content)\n",
    "workflow.add_node('model_answer_gen' , draft_model_answer)\n",
    "\n",
    "#edges \n",
    "workflow.set_entry_point('input')\n",
    "workflow.add_edge('input' , 'ocr')\n",
    "workflow.add_edge(\"ocr\", \"keypoint_gen\")\n",
    "workflow.add_edge('ocr' , 'grammer_report')\n",
    "workflow.add_edge(\"keypoint_gen\", \"policy_search\")\n",
    "workflow.add_edge('policy_search' , 'final_report')\n",
    "\n",
    "workflow.add_conditional_edges('final_report', routing_condition , {True: 'rag', False: END})\n",
    "workflow.add_edge('rag' , 'model_answer_gen')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
